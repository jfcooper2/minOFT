{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209bea0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-13 22:35:37,592] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from minlora import add_lora, apply_to_lora, disable_lora, enable_lora, get_lora_params, merge_lora, name_is_lora, remove_lora, load_multiple_lora, select_lora, get_lora_state_dict\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb6680b-659e-485f-9249-f29ed0686173",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Timing trials; monarch (ABx) vs (ABI)x '''\n",
    "import time\n",
    "import torch\n",
    "from src.models.layers.monarch_linear import MonarchLinear\n",
    "\n",
    "n_runs = 10_000\n",
    "in_features=1000\n",
    "out_features=500\n",
    "device=\"cuda\"\n",
    "monarch = MonarchLinear(nblocks=4, adapt=False, in_features=in_features, out_features=out_features,\n",
    "              bias=False, device=device, dtype=None).to(device)\n",
    "x = torch.rand((2*in_features,in_features),device=device)\n",
    "\n",
    "# start = time.time()\n",
    "# for _ in range(n_runs):\n",
    "#     monarch.forward_matmul(x)\n",
    "# print(time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# for _ in range(n_runs):\n",
    "#     monarch.forward_matmul(torch.eye(in_features,device=device)) @ x\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a644ff6c-9a41-438b-8218-2899bc018016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2*in_features,in_features),device=device)\n",
    "monarch.forward_matmul(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492093a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0967,  0.0831, -0.0068, -0.4254, -0.1586,  0.4504, -0.1301,  0.0330,\n",
      "         -0.0103,  0.1027,  0.4141, -0.2607, -0.3417,  0.1599, -0.0211, -0.0440,\n",
      "         -0.1365, -0.2341,  0.1662,  0.5770, -0.1103,  0.2123, -0.4326,  0.0049,\n",
      "         -0.4706,  0.1912,  0.2688,  0.0759,  1.2353,  0.0486,  0.5336, -0.3987,\n",
      "         -0.0479, -0.7208,  0.2263,  0.4558,  0.0864, -0.2018,  0.2305,  0.5532]])\n"
     ]
    }
   ],
   "source": [
    "# a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=100, out_features=200),\n",
    "    torch.nn.Linear(in_features=200, out_features=40),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 100)\n",
    "y = model(x)\n",
    "print(y)\n",
    "Y0 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98584a8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias False\n",
      "0.parametrizations.weight.original False\n",
      "0.parametrizations.weight.0.monarch.bias False\n",
      "0.parametrizations.weight.0.monarch.blkdiag1 True\n",
      "0.parametrizations.weight.0.monarch.blkdiag2 True\n",
      "1.bias False\n",
      "1.parametrizations.weight.original False\n",
      "1.parametrizations.weight.0.monarch.bias False\n",
      "1.parametrizations.weight.0.monarch.blkdiag1 True\n",
      "1.parametrizations.weight.0.monarch.blkdiag2 True\n"
     ]
    }
   ],
   "source": [
    "# add lora to the model\n",
    "# becase B is initialized to 0, the output is the same as before\n",
    "add_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y0)\n",
    "\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, name_is_lora(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0251891",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1344, -0.0259,  0.0673, -0.3784, -0.1344,  0.5476, -0.0929,  0.0768,\n",
      "         -0.0510,  0.0317,  0.3515, -0.2797, -0.4501,  0.1968, -0.0089, -0.1401,\n",
      "         -0.2008, -0.3028,  0.0180,  0.5884, -0.1402,  0.1190, -0.4799, -0.0457,\n",
      "         -0.4249,  0.0741,  0.2738,  0.1504,  1.1864, -0.0498,  0.6305, -0.3022,\n",
      "          0.0044, -0.6683,  0.1302,  0.4625,  0.1276, -0.1924,  0.2804,  0.6926]])\n"
     ]
    }
   ],
   "source": [
    "# to make the output different, we need to initialize B to something non-zero\n",
    "# model.apply(apply_to_lora(lambda x: torch.nn.init.ones_(x.lora_B)))\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.ones_(x.monarch.blkdiag2)))\n",
    "y = model(x)\n",
    "print(y)\n",
    "assert not torch.allclose(y, Y0)\n",
    "Y1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196087bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# now let's try to disable lora, the output is the same as before lora is added\n",
    "disable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9cba3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# enable lora again\n",
    "enable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f19300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0.parametrizations.weight.0.monarch.blkdiag1', '0.parametrizations.weight.0.monarch.blkdiag2', '1.parametrizations.weight.0.monarch.blkdiag1', '1.parametrizations.weight.0.monarch.blkdiag2'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's save the state dict for later use\n",
    "state_dict_to_save = get_lora_state_dict(model)\n",
    "state_dict_to_save.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9482c292-0b68-4b89-985f-f8c3f2da959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function minlora.utils.name_is_lora(name)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_is_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a06b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can remove lora from the model\n",
    "remove_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522e71f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# lets try to load the lora back\n",
    "# first we need to add lora to the model\n",
    "add_lora(model)\n",
    "# then we can load the lora parameters\n",
    "# strict=False is needed because we are loading a subset of the parameters\n",
    "_ = model.load_state_dict(state_dict_to_save, strict=False) \n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0c8570",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# we can merge it to make it a normal linear layer, so there is no overhead for inference\n",
    "merge_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee283143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (1): Linear(in_features=200, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model now has no lora parameters\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3c246e1",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfaee1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=5, out_features=3)\n",
    "# Step 1: Add LoRA to the model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Collect the parameters, pass them to the optimizer\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": list(get_lora_params(model))},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "\n",
    "# Step 3: Train the model\n",
    "# ...\n",
    "# simulate training, update the LoRA parameters\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.monarch.blkdiag1)))\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.monarch.blkdiag2)))\n",
    "\n",
    "# Step 4: export the LoRA parameters\n",
    "state_dict = model.state_dict()\n",
    "lora_state_dict = {k: v for k, v in state_dict.items() if name_is_lora(k)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "539e7d19",
   "metadata": {},
   "source": [
    "## Loading and Inferencing with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a9836de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add LoRA to your model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Load the LoRA parameters\n",
    "_ = model.load_state_dict(lora_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Merge the LoRA parameters into the model\n",
    "merge_lora(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccba9d68",
   "metadata": {},
   "source": [
    "## Inferencing with multiple LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ef4b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LoRAParametrization' object has no attribute 'lora_A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m lora_state_dict_2 \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mzeros_like(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m lora_state_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     12\u001b[0m lora_state_dicts \u001b[38;5;241m=\u001b[39m [lora_state_dict_0, lora_state_dict_1, lora_state_dict_2]\n\u001b[0;32m---> 14\u001b[0m \u001b[43mload_multiple_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_state_dicts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 3: Select which LoRA to use at inference time\u001b[39;00m\n\u001b[1;32m     17\u001b[0m Y0 \u001b[38;5;241m=\u001b[39m select_lora(model, \u001b[38;5;241m0\u001b[39m)(x)\n",
      "File \u001b[0;32m/workspace/private/minLoRA/minlora/utils.py:75\u001b[0m, in \u001b[0;36mload_multiple_lora\u001b[0;34m(model, lora_state_dicts)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state_dict \u001b[38;5;129;01min\u001b[39;00m lora_state_dicts:\n\u001b[1;32m     74\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_to_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_append_lora\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:898\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    897\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 898\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/private/minLoRA/minlora/utils.py:10\u001b[0m, in \u001b[0;36mapply_to_lora.<locals>.apply_fn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_fn\u001b[39m(layer):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, LoRAParametrization):\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/private/minLoRA/minlora/utils.py:67\u001b[0m, in \u001b[0;36m_append_lora\u001b[0;34m(lora_layer)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_append_lora\u001b[39m(lora_layer):\n\u001b[0;32m---> 67\u001b[0m     lora_layer\u001b[38;5;241m.\u001b[39mlora_As\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mlora_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_A\u001b[49m\u001b[38;5;241m.\u001b[39mclone()))\n\u001b[1;32m     68\u001b[0m     lora_layer\u001b[38;5;241m.\u001b[39mlora_Bs\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mParameter(lora_layer\u001b[38;5;241m.\u001b[39mlora_B\u001b[38;5;241m.\u001b[39mclone()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoRAParametrization' object has no attribute 'lora_A'"
     ]
    }
   ],
   "source": [
    "# to avoid re-adding lora to the model when rerun the cell, remove lora first \n",
    "remove_lora(model)\n",
    "# Step 1: Add LoRA to your model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Load the LoRA parameters\n",
    "\n",
    "# fake 3 sets of LoRA parameters\n",
    "lora_state_dict_0 = lora_state_dict\n",
    "lora_state_dict_1 = {k: torch.ones_like(v) for k, v in lora_state_dict.items()}\n",
    "lora_state_dict_2 = {k: torch.zeros_like(v) for k, v in lora_state_dict.items()}\n",
    "lora_state_dicts = [lora_state_dict_0, lora_state_dict_1, lora_state_dict_2]\n",
    "\n",
    "load_multiple_lora(model, lora_state_dicts)\n",
    "\n",
    "# Step 3: Select which LoRA to use at inference time\n",
    "Y0 = select_lora(model, 0)(x)\n",
    "Y1 = select_lora(model, 1)(x)\n",
    "Y2 = select_lora(model, 2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67602a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0, Y1, Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lora(model)\n",
    "init_state_dict = model.state_dict()\n",
    "# verify that it's the same as if we load the lora parameters one by one\n",
    "for state_dict in lora_state_dicts:\n",
    "    remove_lora(model)\n",
    "    _ = model.load_state_dict(init_state_dict, strict=False)\n",
    "    add_lora(model)\n",
    "    _ = model.load_state_dict(state_dict, strict=False)\n",
    "    merge_lora(model)\n",
    "    y = model(x)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
