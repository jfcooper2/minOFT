{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc63042a-d74e-434b-b9b8-0654f5a58bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "305b535c-59a6-42d7-a18f-1c5120c4b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de0d60dc-dec7-4ae1-a486-4b7ad8630f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "RobertaModel                                            --\n",
       "├─RobertaEmbeddings: 1-1                                --\n",
       "│    └─Embedding: 2-1                                   38,603,520\n",
       "│    └─Embedding: 2-2                                   394,752\n",
       "│    └─Embedding: 2-3                                   768\n",
       "│    └─LayerNorm: 2-4                                   1,536\n",
       "│    └─Dropout: 2-5                                     --\n",
       "├─RobertaEncoder: 1-2                                   --\n",
       "│    └─ModuleList: 2-6                                  --\n",
       "│    │    └─RobertaLayer: 3-1                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-2                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-3                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-4                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-5                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-6                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-7                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-8                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-9                           7,087,872\n",
       "│    │    └─RobertaLayer: 3-10                          7,087,872\n",
       "│    │    └─RobertaLayer: 3-11                          7,087,872\n",
       "│    │    └─RobertaLayer: 3-12                          7,087,872\n",
       "├─RobertaPooler: 1-3                                    --\n",
       "│    └─Linear: 2-7                                      590,592\n",
       "│    └─Tanh: 2-8                                        --\n",
       "================================================================================\n",
       "Total params: 124,645,632\n",
       "Trainable params: 124,645,632\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model,shape=(4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "474df2e8-7f0c-4dd8-8e4a-b573e82adf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'transformers.models.roberta.modeling_roberta.RobertaModel'>\n",
      "embeddings <class 'transformers.models.roberta.modeling_roberta.RobertaEmbeddings'>\n",
      "embeddings.word_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.position_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.token_type_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "embeddings.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder <class 'transformers.models.roberta.modeling_roberta.RobertaEncoder'>\n",
      "encoder.layer <class 'torch.nn.modules.container.ModuleList'>\n",
      "encoder.layer.0 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.0.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.0.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.0.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.0.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.0.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.0.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.0.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.1.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.1.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.1.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.1.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.1.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.1.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.1.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.2.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.2.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.2.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.2.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.2.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.2.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.2.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.3.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.3.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.3.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.3.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.3.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.3.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.3.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.4.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.4.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.4.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.4.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.4.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.4.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.4.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.5.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.5.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.5.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.5.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.5.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.5.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.5.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.6.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.6.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.6.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.6.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.6.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.6.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.6.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.7.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.7.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.7.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.7.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.7.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.7.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.7.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.8.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.8.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.8.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.8.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.8.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.8.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.8.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.9.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.9.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.9.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.9.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.9.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.9.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.9.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.10.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.10.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.10.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.10.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.10.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.10.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.10.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.11.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.11.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.11.attention.self.query <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.key <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.value <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.11.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.11.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.11.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.11.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "pooler <class 'transformers.models.roberta.modeling_roberta.RobertaPooler'>\n",
      "pooler.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "pooler.activation <class 'torch.nn.modules.activation.Tanh'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b054f8a3-3915-4641-9817-85e6260d129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") query Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  query\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") key Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  key\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfAttention(\n",
      "  (query): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (key): MonarchInjectedLinear_2(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (monarch): MonarchLinear()\n",
      "  )\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") value Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  value\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "RobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") dense Linear(in_features=768, out_features=768, bias=True)\n",
      "Monarch LoRA Injection : injecting monarch into  dense\n",
      "Monarch LoRA Injection : weight shape torch.Size([768, 768])\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "from minoft.modular_monarch import inject_trainable_monarch, monkeypatch_remove_lora\n",
    "import torch\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Set OFT parameters\n",
    "oft_r=4\n",
    "oft_eps=1e-3\n",
    "oft_coft=False\n",
    "oft_block_share=False\n",
    "normalize=False\n",
    "search_class=[torch.nn.Linear] # Default is only nn.Linear, but you can also pass nn.Conv2d\n",
    "\n",
    "# Set training and optimization parameters\n",
    "learning_rate=2e-5\n",
    "weight_decay=0.01\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "\n",
    "# Replace modules with trainable OFT linear modules\n",
    "ft_modules = [\"RobertaAttention\"] # Modules will be specific to your model, but you can target any number of them\n",
    "oft_params, train_names = inject_trainable_monarch(\n",
    "                              model, \n",
    "                              verbose=True, \n",
    "                              target_replace_module = ft_modules\n",
    "                          )\n",
    "# Set optimizer\n",
    "optim_groups = [\n",
    "    {\n",
    "        \"params\": oft_params,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "print(len(oft_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af2849d7-a229-4035-8ebd-48b7f7351daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'transformers.models.roberta.modeling_roberta.RobertaModel'>\n",
      "embeddings <class 'transformers.models.roberta.modeling_roberta.RobertaEmbeddings'>\n",
      "embeddings.word_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.position_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.token_type_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "embeddings.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder <class 'transformers.models.roberta.modeling_roberta.RobertaEncoder'>\n",
      "encoder.layer <class 'torch.nn.modules.container.ModuleList'>\n",
      "encoder.layer.0 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.0.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.0.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.0.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.0.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.0.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.0.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.0.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.1.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.1.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.1.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.1.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.1.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.1.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.1.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.2.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.2.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.2.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.2.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.2.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.2.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.2.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.3.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.3.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.3.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.3.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.3.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.3.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.3.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.4.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.4.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.4.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.4.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.4.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.4.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.4.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.5.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.5.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.5.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.5.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.5.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.5.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.5.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.6.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.6.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.6.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.6.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.6.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.6.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.6.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.7.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.7.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.7.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.7.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.7.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.7.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.7.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.8.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.8.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.8.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.8.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.8.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.8.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.8.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.9.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.9.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.9.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.9.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.9.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.9.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.9.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.10.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.10.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.10.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.10.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.10.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.10.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.10.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.11.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.11.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.11.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.11.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.11.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.11.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.11.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "pooler <class 'transformers.models.roberta.modeling_roberta.RobertaPooler'>\n",
      "pooler.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "pooler.activation <class 'torch.nn.modules.activation.Tanh'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa43b74-8412-4716-8966-77edd5e7298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "RobertaModel                                                 --\n",
       "├─RobertaEmbeddings: 1-1                                     --\n",
       "│    └─Embedding: 2-1                                        (38,603,520)\n",
       "│    └─Embedding: 2-2                                        (394,752)\n",
       "│    └─Embedding: 2-3                                        (768)\n",
       "│    └─LayerNorm: 2-4                                        (1,536)\n",
       "│    └─Dropout: 2-5                                          --\n",
       "├─RobertaEncoder: 1-2                                        --\n",
       "│    └─ModuleList: 2-6                                       --\n",
       "│    │    └─RobertaLayer: 3-1                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-2                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-3                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-4                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-5                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-6                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-7                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-8                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-9                                8,267,520\n",
       "│    │    └─RobertaLayer: 3-10                               8,267,520\n",
       "│    │    └─RobertaLayer: 3-11                               8,267,520\n",
       "│    │    └─RobertaLayer: 3-12                               8,267,520\n",
       "├─RobertaPooler: 1-3                                         --\n",
       "│    └─Linear: 2-7                                           (590,592)\n",
       "│    └─Tanh: 2-8                                             --\n",
       "=====================================================================================\n",
       "Total params: 138,801,408\n",
       "Trainable params: 14,155,776\n",
       "Non-trainable params: 124,645,632\n",
       "====================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model,shape=(4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e54d7fbb-965d-4955-b1e1-24bcd24d120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeypatch_remove_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d0b475d-65ab-4f1d-9a4a-03c9f66f7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'transformers.models.roberta.modeling_roberta.RobertaModel'>\n",
      "embeddings <class 'transformers.models.roberta.modeling_roberta.RobertaEmbeddings'>\n",
      "embeddings.word_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.position_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.token_type_embeddings <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "embeddings.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder <class 'transformers.models.roberta.modeling_roberta.RobertaEncoder'>\n",
      "encoder.layer <class 'torch.nn.modules.container.ModuleList'>\n",
      "encoder.layer.0 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.0.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.0.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.0.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.0.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.0.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.0.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.0.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.0.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.0.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.1.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.1.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.1.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.1.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.1.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.1.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.1.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.1.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.1.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.2.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.2.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.2.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.2.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.2.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.2.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.2.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.2.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.2.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.3.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.3.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.3.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.3.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.3.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.3.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.3.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.3.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.3.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.4.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.4.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.4.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.4.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.4.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.4.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.4.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.4.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.4.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.5.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.5.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.5.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.5.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.5.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.5.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.5.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.5.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.5.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.6.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.6.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.6.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.6.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.6.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.6.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.6.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.6.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.6.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.7.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.7.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.7.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.7.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.7.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.7.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.7.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.7.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.7.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.8.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.8.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.8.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.8.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.8.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.8.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.8.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.8.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.8.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.9.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.9.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.9.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.9.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.9.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.9.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.9.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.9.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.9.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.10.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.10.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.10.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.10.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.10.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.10.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.10.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.10.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.10.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11 <class 'transformers.models.roberta.modeling_roberta.RobertaLayer'>\n",
      "encoder.layer.11.attention <class 'transformers.models.roberta.modeling_roberta.RobertaAttention'>\n",
      "encoder.layer.11.attention.self <class 'transformers.models.roberta.modeling_roberta.RobertaSelfAttention'>\n",
      "encoder.layer.11.attention.self.query <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.query.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.query.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.query.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.key <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.key.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.key.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.key.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.value <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.self.value.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.self.value.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.value.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.self.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output <class 'transformers.models.roberta.modeling_roberta.RobertaSelfOutput'>\n",
      "encoder.layer.11.attention.output.dense <class 'minoft.modular_monarch.MonarchInjectedLinear_2'>\n",
      "encoder.layer.11.attention.output.dense.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output.dense.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.output.dense.monarch <class 'minoft.src.models.layers.monarch_linear.MonarchLinear'>\n",
      "encoder.layer.11.attention.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.intermediate <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>\n",
      "encoder.layer.11.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.11.output <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>\n",
      "encoder.layer.11.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.output.LayerNorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "pooler <class 'transformers.models.roberta.modeling_roberta.RobertaPooler'>\n",
      "pooler.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "pooler.activation <class 'torch.nn.modules.activation.Tanh'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module.__class__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
